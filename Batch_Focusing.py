import tensorflow as tf
import numpy as np

def ContentFocusing(k_t, M_prev, beta_t, K = None):
    
    '''
    Computes the Write Vector for whole batch through Content Attentioning in one go.
    
    k_t : (Batch_size,M), Key Vector generated by EITHER HEAD (in whichever HEAD this function is used in for addressing)
    M_prev : (Batch_size,N,M), Memory Matrix at time t.
    beta_t : (Batch_size,1), Key Strength hyperparameter
    K : Function, Similarity Measure, if None, Cosine Similarity will be used.
    
    RETURNS:
    
    w_ct : (Batch_size,N), Weighting after Content Focusing. 
    '''
    
    batch_size,N,M = M_prev.shape
    

    
    if K != None : 
            
        K_kt_Mprev = K(k_t,M_prev)
    
    else:
        K_kt_Mprev = tf.reduce_sum(tf.reshape(k_t,(batch_size,1,M))*M_prev, axis = 2) /  tf.multiply(tf.reshape(tf.linalg.norm(k_t,axis =1),(batch_size,1)),tf.linalg.norm(M_prev,axis = 2)) 
        #^ Of shape [batch_size, N]
    exp_vals = tf.exp(beta_t*K_kt_Mprev)
    w_ct = exp_vals / tf.reshape(tf.reduce_sum(exp_vals,axis = 1),(-1,1)) 
    
    assert w_ct.shape == (batch_size,N)
    
    return w_ct


def LocationFocusing( k_t, M_prev, beta_t,    g_t, w_prev, s_t, gamma_t,   K = None):
    
    '''
    Computes the Write Vector for whole batch through Location Attentioning in one go.
    
    k_t, M_prev, b_t, K : SAME AS IN CONTENT FOCUSING
    g_t : (batch_size,1), Interpolation Gate in the range (0,1) emitted by HEAD IN USE.
    w_prev : (batch_size,N), Weight Vector produced by the HEAD IN USE at the previous time step.
    s_t : (batch_size, len(shift_range)), The weights emitted by the HEAD IN USE that defines the normalized distribution over the allowed integer shifts (which is shift_range object)
                
    gamma_t : (batch_size,1), Sharpening Factor >= 1    
    
    RETURNS:
    
    w_t : (batch_size,N), Final Weight Vector through Location Attentioning
    '''
    
    w_ct = ContentFocusing(k_t, M_prev, beta_t, K)
    
    batch_size,N,M = M_prev.shape
    
    #assert w_prev.shape == (N,)
    
    #Interpolation
    w_gt = g_t * w_ct + (1 - g_t) * w_prev
    
    #Convolutional Shift
        #The main Hurdle!!
    w_hat_t = tf.concat([Convolution(s_t[i],w_gt[i]) for i in range(batch_size)],axis = 0)
    #^Of shape [batch_size, N]
    
    
    #Sharpening
    powered = tf.pow(w_hat_t, g_t)
    w_t = powered / tf.reshape(tf.reduce_sum(powered, axis = 1),(-1,1))
    
    return w_t


def Convolution(s_t, w_gt):
    '''
    The Circular Convolutor.
    
    s_t: (len(shift_range),) ; Shift Weighting of the particualar input
    w_gt: (N,) ; W_gt Vector for the particualar input as calculated in the Focusing function
    
    RETURNS:
    
    w_hat_t: (1,N) ; Vector found after Circular Convolution of s_t on w_gt
    '''
    LSR = len(s_t) #length of shift range
    N = w_gt.shape[0]
    test = []
    for i in range(LSR):
        test.append([j for j in range(i+1, 2*N + i, 2)])
    repeat_matrix = tf.transpose(tf.convert_to_tensor(test))
    if N%LSR != 0:
        repeat_matrix = tf.concat([repeat_matrix,tf.transpose(test[:N%LSR])],axis = 1)

    index_mat = np.array(repeat_matrix%LSR,dtype = np.float32)

    for i in range(LSR):
        index_mat[index_mat == (i+1)%LSR] = s_t[i]

    res = tf.matmul(tf.reshape(w_gt,(1,-1)),index_mat)

    final_result = tf.concat([res for _ in range(int(N/LSR))], axis = 1)
    if N%LSR != 0:
        final_result = tf.concat([final_result,res[:,:N%LSR]], axis = 1)
        
    return final_result