{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Turing Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focusing and Weights Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Focusing By Content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ContentFocusing(k_t, M_t, b_t, K = None):\n",
    "    \n",
    "    '''\n",
    "    k_t : (M,), Key Vector generated by EITHER HEAD (in whichever HEAD this function is used in for addressing)\n",
    "    M_t : (N,M), Memory Matrix at time t.\n",
    "    b_t : Scalar, Key Strength hyperparameter\n",
    "    K : Function, Similarity Measure, if None, Cosine Similarity will be used.\n",
    "    \n",
    "    RETURNS:\n",
    "    \n",
    "    w_ct : (N,), Weighting after Content Focusing. \n",
    "    '''\n",
    "    \n",
    "    N,M = M_t.shape\n",
    "    \n",
    "    assert k_t.shape == (M,)\n",
    "    \n",
    "    if K == None : \n",
    "        \n",
    "        def Cosine_Similarity(u,v):\n",
    "            u,v = tf.reshape(u,(1,-1)),tf.reshape(v,(-1,1))\n",
    "            return np.dot(u,v)/(np.linalg.norm(u)*np.linalg.norm(v))\n",
    "        K = Cosine_Similarity\n",
    "        \n",
    "    Applied_K_Vector = np.apply_along_axis(K,1,M_t,tf.reshape(k_t,(1,M))).reshape(-1,1)\n",
    "    exp_of_AKV = tf.exp(b_t * Applied_K_Vector)                             #AKV for Applied K Vector\n",
    "    w_ct = exp_of_AKV/np.sum(exp_of_AKV)\n",
    "    \n",
    "    assert w_ct.shape == (N,1)\n",
    "    \n",
    "    return tf.reshape(w_ct,(N,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "M = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_t = tf.random.uniform((M,))\n",
    "b_t = 1\n",
    "M_t = tf.random.uniform((N,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=23, shape=(10, 5), dtype=float32, numpy=\n",
       "array([[0.15168643, 0.5299089 , 0.42401373, 0.8561846 , 0.02457011],\n",
       "       [0.75537217, 0.8290411 , 0.14860976, 0.79608643, 0.41153967],\n",
       "       [0.9069629 , 0.89466023, 0.4181559 , 0.29921603, 0.1102289 ],\n",
       "       [0.68157077, 0.07642555, 0.7616246 , 0.52278054, 0.7089381 ],\n",
       "       [0.71060276, 0.20000505, 0.753319  , 0.6242901 , 0.7561432 ],\n",
       "       [0.85267484, 0.46078706, 0.18122256, 0.8119124 , 0.91077876],\n",
       "       [0.30243218, 0.82506585, 0.6806989 , 0.52212954, 0.58182204],\n",
       "       [0.9636531 , 0.73690915, 0.04979634, 0.8495326 , 0.7736132 ],\n",
       "       [0.10018981, 0.17872894, 0.42052138, 0.64439833, 0.2032851 ],\n",
       "       [0.69255817, 0.20251226, 0.1986072 , 0.4592415 , 0.598354  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=16, shape=(5,), dtype=float32, numpy=\n",
       "array([0.2839321 , 0.66500413, 0.9848646 , 0.6066334 , 0.02315331],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=97, shape=(10,), dtype=float32, numpy=\n",
       "array([0.11594453, 0.09759745, 0.10331585, 0.09827673, 0.10120873,\n",
       "       0.08538678, 0.11552462, 0.08628917, 0.11229491, 0.0841612 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ContentFocusing(k_t, M_t, b_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Focusing By Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LocationFocusing( k_t, M_t, b_t,    g_t, w_prev, s_t, gamma_t,   K = None,):\n",
    "    \n",
    "    '''\n",
    "    k_t, M_t, b_t, K : SAME AS IN CONTENT FOCUSING\n",
    "    g_t : Scalar, Interpolation Gate in the range (0,1) emitted by HEAD IN USE.\n",
    "    w_prev : (N,), Weight Vector produced by the HEAD IN USE at the previous time step.\n",
    "    s_t : (N,), The weights emitted by the HEAD IN USE that defines the normalized distribution over the allowed integer shifts (which is shift_range object)\n",
    "                NOTE: s_t is supposed to be padded by zeroes for all the elements not in the shift_range, thus making it a length N vector from length len(shift_range) vector.\n",
    "    gamma_t : Scalar, Sharpening Factor >= 1    \n",
    "    \n",
    "    RETURNS:\n",
    "    \n",
    "    w_t : (N,), Final Weight Vector \n",
    "    '''\n",
    "    \n",
    "    w_ct = ContentFocusing(k_t, M_t, b_t, K)\n",
    "    N,M = M_t.shape\n",
    "    \n",
    "    assert w_prev.shape == (N,)\n",
    "    \n",
    "    #Interpolation\n",
    "    w_gt = g_t * w_ct + (1 - g_t) * w_prev\n",
    "    \n",
    "    #Convolutional Shift\n",
    "    w_hat_t = np.zeros(N)                       #These loops will limit the speed clearly, it would be good to wrap them in C (or find an alternative function)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            w_hat_t[i] += w_gt[j]*s_t[i-j]\n",
    "    \n",
    "    #Sharpening\n",
    "    powered = tf.pow(w_hat_t,gamma_t)\n",
    "    w_t = powered/np.sum(powered)\n",
    "    \n",
    "    return w_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_t = 0.8\n",
    "w_prev = tf.random.uniform((N,))\n",
    "s_t = tf.random.uniform((N,))\n",
    "gamma_t = 2.4423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=62863, shape=(10,), dtype=float64, numpy=\n",
       "array([0.11293619, 0.09017039, 0.11173104, 0.08184061, 0.07534759,\n",
       "       0.1112131 , 0.0743389 , 0.11282455, 0.1379482 , 0.09164943])>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LocationFocusing( k_t, M_t, b_t,    g_t, w_prev, s_t, gamma_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
