{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Focusing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ReadWriteVectorGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLEASE NOTE TO IMPLEMENT INTIAL STATES AS tf.float64 TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTMCell(tf.keras.layers.AbstractRNNCell):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        ARGUMENTS TO THE call FUNCTION::--\n",
    "        \n",
    "        ////////////////////          TODO       /////////////////////////          \n",
    "        \n",
    "        Few Points:\n",
    "        So the combination of inputs and preious_states will be used to calculate w_t, this w_t will be used for READING, the resulting vector r_t will be outputted.\n",
    "        The same w_t is also used for Writing Purposes, which results in a updated Memory Matrix, which is also returned as the next_state's element.\n",
    "        \n",
    "        \n",
    "        ////////////////////// TODO  //////////////////////////\n",
    "        \n",
    "        \n",
    "        MORE ELABORATION ON THE METHODOLOGY NEEDS TO BE DONE \n",
    "        \n",
    "        \n",
    "    '''\n",
    "    \n",
    "    #the Initialiser function\n",
    "    def __init__(self, rnn_size, memory_rows, memory_columns, num_read_heads, num_write_heads, num_bits_per_output_vector, addressing_type = 'LOC', shift_range = tf.range(-1,2), controller = tf.keras.layers.LSTMCell \n",
    "                     ): \n",
    "        #output_dim: Number of bits per output vector\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        \n",
    "        self.rnn_size = rnn_size\n",
    "        self.memory_rows = memory_rows       #The \"N\" or \"size of memory\" in Literature\n",
    "        self.memory_columns = memory_columns      #The \"M\" or \"memory vector's dimension\" in Literature\n",
    "        self.num_read_heads = num_read_heads\n",
    "        self.num_write_heads = num_write_heads\n",
    "        self.num_bits_per_output_vector = num_bits_per_output_vector\n",
    "        self.addressing_type = addressing_type\n",
    "        \n",
    "        if ((self.addressing_type != 'LOC') or (self.addressing_type != 'CONT')):\n",
    "            raise ValueError('Incorrect Addressing Type: Allowed values are \"LOC\" for Location based Focusing and \"CONT\" for Content based Focusing.')\n",
    "\n",
    "        \n",
    "        self.shift_range = shift_range\n",
    "        \n",
    "            \n",
    "            \n",
    "        #self.state_size TODO:::: Define a @property for this which returns the dictionary containing the states as discussed.\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.output_size = self.num_bits_per_output_vector\n",
    "        self.parameter_count = self.memory_columns + 1 + 1 + len(self.shift_range) + 1 + self.memory_columns + self.memory_columns   #As Discussed\n",
    "            \n",
    "        \n",
    "        self.controller = controller(self.rnn_size)     #The controller to use\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.total_num_heads = self.num_read_heads + self.num_write_heads\n",
    "        \n",
    "        self.controller_to_parameter = tf.keras.layers.Dense(units = self.parameter_count,use_bias=True)\n",
    "        \n",
    "        self.parameter_matrix_to_NTM_output = tf.keras.layers.Dense(units = self.output_size,use_bias=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "    #The call function which will be called at each time/input step\n",
    "    def call(self, inputs, previous_state):\n",
    "    \n",
    "        prev_read_vectors = previous_state['current_read_vectors']\n",
    "        \n",
    "        prev_controller_state = previous_state['current_controller_state']\n",
    "        \n",
    "        input_to_controller = tf.concat([inputs] + prev_read_vectors, axis = 1)\n",
    "        \n",
    "        controller_output, controller_state = self.controller(input_to_controller, prev_controller_state)\n",
    "        \n",
    "        parameter_matrix = self.controller_to_parameter(controller_output)\n",
    "        \n",
    "        \n",
    "        M_prev = previous_state['Memory_Matrix']\n",
    "        \n",
    "        w_prev_list = previous_state['current_weights_list']\n",
    "        \n",
    "        \n",
    "        # Creating a list of all Parameters Emitted by the Heads\n",
    "        controller_parameter_list = tf.split(parameter_matrix[0],[self.memory_columns ,1, 1, len(self.shift_range), 1, self.memory_columns, self.memory_columns], axis = 1)\n",
    "        # Applying Layers on the parameter Matrix as per the Paramter's Location.\n",
    "        \n",
    "        \n",
    "        #Useless Shape check\n",
    "        assert self.total_num_heads == controller_parameter_list[0].shape[0]\n",
    "        \n",
    "        \n",
    "        #For k_t:-\n",
    "        controller_parameter_list[0] = tf.tanh(controller_parameter_list[0])\n",
    "        k_t = controller_parameter_list[0]\n",
    "        #For beta_t:-\n",
    "        controller_parameter_list[1] = tf.sigmoid(controller_parameter_list[1]) * 5 #Attenuating Factor\n",
    "        beta_t = controller_parameter_list[1]\n",
    "        #For g_t:-\n",
    "        controller_parameter_list[2] = tf.sigmoid(controller_parameter_list[2])\n",
    "        g_t = controller_parameter_list[2]\n",
    "        #For s_t:-\n",
    "        controller_parameter_list[3] = tf.nn.softmax(controller_parameter_list[3], axis = 1)\n",
    "        s_t = controller_parameter_list[3]\n",
    "        #The above s_t is one of the points where we can improve\n",
    "        #For gamma_t:-\n",
    "        controller_parameter_list[4] = tf.math.log(controller_parameter_list[4]) + 1 #Since gamma >= 1\n",
    "        gamma_t = controller_parameter_list[4]\n",
    "        #For a_t:-\n",
    "        controller_parameter_list[5] = tf.tanh(controller_parameter_list[5])\n",
    "        a_t = controller_parameter_list[5]\n",
    "        #For e_t:-\n",
    "        controller_parameter_list[6] = tf.sigmoid(controller_parameter_list[6])\n",
    "        e_t = controller_parameter_list[6]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Finally, we find the w_t at this step.\n",
    "        \n",
    "        All_Heads_W = []\n",
    "        \n",
    "        if self.addressing_type == 'LOC':\n",
    "            for i in range(self.total_num_heads):\n",
    "                w_t = Focusing.LocationFocusing(k_t[i], M_prev, beta_t[i], g_t[i], w_prev_list[i],s_t[i], gamma_t[i]) #Note that Cosine Similarity is used\n",
    "                All_Heads_W.append(w_t)\n",
    "            \n",
    "        elif self.addressing_type == 'CONT':\n",
    "            for i in range(self.total_num_heads):\n",
    "                w_t = Focusing.ContentFocusing(k_t[i], M_prev, beta_t[i]) #Note that Cosine Similarity is used\n",
    "                All_Heads_W.append(w_t)\n",
    "                \n",
    "        #It would be better if we find a vectorized way to go about it.\n",
    "        \n",
    "        #Now for Writing and Reading\n",
    "        #NOTE: We consider first self.num_read_heads as READ HEADS and the rest as WRITE HEADS\n",
    "        \n",
    "        read_Heads_w = All_Heads_W[:self.num_read_heads]\n",
    "        write_Heads_w = All_Heads_W[self.num_read_heads:]\n",
    "        \n",
    "        assert len(write_Heads_w) == self.num_write_heads\n",
    "        \n",
    "        r_t_list = []\n",
    "        for read_head_w in read_Heads_w:\n",
    "            r_t = ReadWriteVectorGeneration.ReadVector(M_prev,read_head_w)\n",
    "            r_t_list.append(r_t)\n",
    "            \n",
    "        for i, write_head_w in enumerate(write_Heads_w):\n",
    "            M_prev = ReadWriteVectorGeneration.WriteOnMemory(M_prev, write_head_w, e_t[self.num_read_heads + i], a_t[self.num_read_heads + i])\n",
    "           #Note that the M_prev is now the updated Memory Matrix by action from all the WRITE HEADS\n",
    "        \n",
    "        assert len(r_t_list) == len(prev_read_vectors)\n",
    "        \n",
    "        \n",
    "        state = {\n",
    "                     'current_read_vectors' : r_t_list,\n",
    "                     'current_controller_state' : controller_state,\n",
    "                     'current_weights_list' : All_Heads_W,\n",
    "                     'Memory_Matrix' : M_prev\n",
    "                }\n",
    "        \n",
    "        \n",
    "        NTM_output = self.parameter_matrix_to_NTM_output(parameter_matrix)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return NTM_output ,state\n",
    "    \n",
    "    def "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough Testing Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Final Weights Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15\n",
    "M = 8\n",
    "HEADS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_t = tf.random.uniform((HEADS,1))\n",
    "w_prev = tf.random.uniform((HEADS,N))\n",
    "s_t = tf.nn.softmax(tf.random.uniform((HEADS,3)),axis = 1)\n",
    "gamma_t = tf.random.uniform((HEADS,1))*2 + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_t = tf.random.uniform((HEADS,M))\n",
    "b_t = tf.random.uniform((HEADS,1))*2 + 1 \n",
    "M_t = tf.random.uniform((N,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Heads_W = []\n",
    "for i in range(HEADS):\n",
    "    w_t = Focusing.LocationFocusing(k_t[i], M_t, b_t[i], g_t[i], w_prev[i],s_t[i], gamma_t[i]) #Note that Cosine Similarity is used\n",
    "    All_Heads_W.append(w_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=106378, shape=(15,), dtype=float32, numpy=\n",
       "array([0.06764598, 0.0660094 , 0.06634461, 0.06764598, 0.0660094 ,\n",
       "       0.06634461, 0.06764598, 0.0660094 , 0.06634461, 0.06764598,\n",
       "       0.0660094 , 0.06634461, 0.06764598, 0.0660094 , 0.06634461],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "Focusing.LocationFocusing(k_t[i], M_t, b_t[i], g_t[i], w_prev[i] ,s_t[i], gamma_t[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=106382, shape=(1,), dtype=float32, numpy=array([2.7387912], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=2698, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06764598, 0.0660094 , 0.06634461, 0.06764598, 0.0660094 ,\n",
       "        0.06634461, 0.06764598, 0.0660094 , 0.06634461, 0.06764598,\n",
       "        0.0660094 , 0.06634461, 0.06764598, 0.0660094 , 0.06634461],\n",
       "       dtype=float32)>, <tf.Tensor: id=5290, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06890429, 0.07049696, 0.06059873, 0.06890429, 0.07049696,\n",
       "        0.06059873, 0.06890429, 0.07049696, 0.06059873, 0.06890429,\n",
       "        0.07049696, 0.06059873, 0.06890429, 0.07049696, 0.06059873],\n",
       "       dtype=float32)>, <tf.Tensor: id=7882, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06615897, 0.06245341, 0.07138763, 0.06615897, 0.06245341,\n",
       "        0.07138763, 0.06615897, 0.06245341, 0.07138763, 0.06615897,\n",
       "        0.06245341, 0.07138763, 0.06615897, 0.06245341, 0.07138763],\n",
       "       dtype=float32)>, <tf.Tensor: id=10474, shape=(15,), dtype=float32, numpy=\n",
       " array([0.0679554 , 0.06770483, 0.06433976, 0.0679554 , 0.06770483,\n",
       "        0.06433976, 0.0679554 , 0.06770483, 0.06433976, 0.0679554 ,\n",
       "        0.06770483, 0.06433976, 0.0679554 , 0.06770483, 0.06433976],\n",
       "       dtype=float32)>, <tf.Tensor: id=13066, shape=(15,), dtype=float32, numpy=\n",
       " array([0.05866151, 0.0722268 , 0.06911168, 0.05866151, 0.0722268 ,\n",
       "        0.06911168, 0.05866151, 0.0722268 , 0.06911168, 0.05866151,\n",
       "        0.0722268 , 0.06911168, 0.05866151, 0.0722268 , 0.06911168],\n",
       "       dtype=float32)>, <tf.Tensor: id=15658, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06639673, 0.06495553, 0.06864774, 0.06639673, 0.06495553,\n",
       "        0.06864774, 0.06639673, 0.06495553, 0.06864774, 0.06639673,\n",
       "        0.06495553, 0.06864774, 0.06639673, 0.06495553, 0.06864774],\n",
       "       dtype=float32)>, <tf.Tensor: id=18250, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06645577, 0.0644941 , 0.06905013, 0.06645577, 0.0644941 ,\n",
       "        0.06905013, 0.06645577, 0.0644941 , 0.06905013, 0.06645577,\n",
       "        0.0644941 , 0.06905013, 0.06645577, 0.0644941 , 0.06905013],\n",
       "       dtype=float32)>, <tf.Tensor: id=20842, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06926727, 0.06343981, 0.0672929 , 0.06926727, 0.06343981,\n",
       "        0.0672929 , 0.06926727, 0.06343981, 0.0672929 , 0.06926727,\n",
       "        0.06343981, 0.0672929 , 0.06926727, 0.06343981, 0.0672929 ],\n",
       "       dtype=float32)>, <tf.Tensor: id=23434, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06668215, 0.07300029, 0.06031753, 0.06668215, 0.07300029,\n",
       "        0.06031753, 0.06668215, 0.07300029, 0.06031753, 0.06668215,\n",
       "        0.07300029, 0.06031753, 0.06668215, 0.07300029, 0.06031753],\n",
       "       dtype=float32)>, <tf.Tensor: id=26026, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06724209, 0.06652641, 0.06623153, 0.06724209, 0.06652641,\n",
       "        0.06623153, 0.06724209, 0.06652641, 0.06623153, 0.06724209,\n",
       "        0.06652641, 0.06623153, 0.06724209, 0.06652641, 0.06623153],\n",
       "       dtype=float32)>, <tf.Tensor: id=28618, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06339911, 0.07129148, 0.06530937, 0.06339911, 0.07129148,\n",
       "        0.06530937, 0.06339911, 0.07129148, 0.06530937, 0.06339911,\n",
       "        0.07129148, 0.06530937, 0.06339911, 0.07129148, 0.06530937],\n",
       "       dtype=float32)>, <tf.Tensor: id=31210, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06404878, 0.06569303, 0.07025818, 0.06404878, 0.06569303,\n",
       "        0.07025818, 0.06404878, 0.06569303, 0.07025818, 0.06404878,\n",
       "        0.06569303, 0.07025818, 0.06404878, 0.06569303, 0.07025818],\n",
       "       dtype=float32)>, <tf.Tensor: id=33802, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06700069, 0.06786614, 0.06513317, 0.06700069, 0.06786614,\n",
       "        0.06513317, 0.06700069, 0.06786614, 0.06513317, 0.06700069,\n",
       "        0.06786614, 0.06513317, 0.06700069, 0.06786614, 0.06513317],\n",
       "       dtype=float32)>, <tf.Tensor: id=36394, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06715818, 0.06663328, 0.06620853, 0.06715818, 0.06663328,\n",
       "        0.06620853, 0.06715818, 0.06663328, 0.06620853, 0.06715818,\n",
       "        0.06663328, 0.06620853, 0.06715818, 0.06663328, 0.06620853],\n",
       "       dtype=float32)>, <tf.Tensor: id=38986, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06827288, 0.06303013, 0.06869698, 0.06827288, 0.06303013,\n",
       "        0.06869698, 0.06827288, 0.06303013, 0.06869698, 0.06827288,\n",
       "        0.06303013, 0.06869698, 0.06827288, 0.06303013, 0.06869698],\n",
       "       dtype=float32)>, <tf.Tensor: id=41578, shape=(15,), dtype=float32, numpy=\n",
       " array([0.07179379, 0.06431486, 0.06389134, 0.07179379, 0.06431486,\n",
       "        0.06389134, 0.07179379, 0.06431486, 0.06389134, 0.07179379,\n",
       "        0.06431486, 0.06389134, 0.07179379, 0.06431486, 0.06389134],\n",
       "       dtype=float32)>, <tf.Tensor: id=44170, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06806644, 0.06661814, 0.0653154 , 0.06806644, 0.06661814,\n",
       "        0.0653154 , 0.06806644, 0.06661814, 0.0653154 , 0.06806644,\n",
       "        0.06661814, 0.0653154 , 0.06806644, 0.06661814, 0.0653154 ],\n",
       "       dtype=float32)>, <tf.Tensor: id=46762, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06114024, 0.06304935, 0.07581039, 0.06114024, 0.06304935,\n",
       "        0.07581039, 0.06114024, 0.06304935, 0.07581039, 0.06114024,\n",
       "        0.06304935, 0.07581039, 0.06114024, 0.06304935, 0.07581039],\n",
       "       dtype=float32)>, <tf.Tensor: id=49354, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06266643, 0.06245301, 0.07488055, 0.06266643, 0.06245301,\n",
       "        0.07488055, 0.06266643, 0.06245301, 0.07488055, 0.06266643,\n",
       "        0.06245301, 0.07488055, 0.06266643, 0.06245301, 0.07488055],\n",
       "       dtype=float32)>, <tf.Tensor: id=51946, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06306572, 0.06742854, 0.06950573, 0.06306572, 0.06742854,\n",
       "        0.06950573, 0.06306572, 0.06742854, 0.06950573, 0.06306572,\n",
       "        0.06742854, 0.06950573, 0.06306572, 0.06742854, 0.06950573],\n",
       "       dtype=float32)>, <tf.Tensor: id=54538, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06612306, 0.06865831, 0.06521866, 0.06612306, 0.06865831,\n",
       "        0.06521866, 0.06612306, 0.06865831, 0.06521866, 0.06612306,\n",
       "        0.06865831, 0.06521866, 0.06612306, 0.06865831, 0.06521866],\n",
       "       dtype=float32)>, <tf.Tensor: id=57130, shape=(15,), dtype=float32, numpy=\n",
       " array([0.07075864, 0.06458863, 0.06465273, 0.07075864, 0.06458863,\n",
       "        0.06465273, 0.07075864, 0.06458863, 0.06465273, 0.07075864,\n",
       "        0.06458863, 0.06465273, 0.07075864, 0.06458863, 0.06465273],\n",
       "       dtype=float32)>, <tf.Tensor: id=59722, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06423494, 0.06952204, 0.06624304, 0.06423494, 0.06952204,\n",
       "        0.06624304, 0.06423494, 0.06952204, 0.06624304, 0.06423494,\n",
       "        0.06952204, 0.06624304, 0.06423494, 0.06952204, 0.06624304],\n",
       "       dtype=float32)>, <tf.Tensor: id=62314, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06771196, 0.06443688, 0.06785115, 0.06771196, 0.06443688,\n",
       "        0.06785115, 0.06771196, 0.06443688, 0.06785115, 0.06771196,\n",
       "        0.06443688, 0.06785115, 0.06771196, 0.06443688, 0.06785115],\n",
       "       dtype=float32)>, <tf.Tensor: id=64906, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06784113, 0.06394266, 0.06821619, 0.06784113, 0.06394266,\n",
       "        0.06821619, 0.06784113, 0.06394266, 0.06821619, 0.06784113,\n",
       "        0.06394266, 0.06821619, 0.06784113, 0.06394266, 0.06821619],\n",
       "       dtype=float32)>, <tf.Tensor: id=67498, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06628475, 0.06440087, 0.0693144 , 0.06628475, 0.06440087,\n",
       "        0.0693144 , 0.06628475, 0.06440087, 0.0693144 , 0.06628475,\n",
       "        0.06440087, 0.0693144 , 0.06628475, 0.06440087, 0.0693144 ],\n",
       "       dtype=float32)>, <tf.Tensor: id=70090, shape=(15,), dtype=float32, numpy=\n",
       " array([0.07044967, 0.06411114, 0.06543921, 0.07044967, 0.06411114,\n",
       "        0.06543921, 0.07044967, 0.06411114, 0.06543921, 0.07044967,\n",
       "        0.06411114, 0.06543921, 0.07044967, 0.06411114, 0.06543921],\n",
       "       dtype=float32)>, <tf.Tensor: id=72682, shape=(15,), dtype=float32, numpy=\n",
       " array([0.0645847 , 0.06817891, 0.06723638, 0.0645847 , 0.06817891,\n",
       "        0.06723638, 0.0645847 , 0.06817891, 0.06723638, 0.0645847 ,\n",
       "        0.06817891, 0.06723638, 0.0645847 , 0.06817891, 0.06723638],\n",
       "       dtype=float32)>, <tf.Tensor: id=75274, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06698303, 0.06650876, 0.06650821, 0.06698303, 0.06650876,\n",
       "        0.06650821, 0.06698303, 0.06650876, 0.06650821, 0.06698303,\n",
       "        0.06650876, 0.06650821, 0.06698303, 0.06650876, 0.06650821],\n",
       "       dtype=float32)>, <tf.Tensor: id=77866, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06542546, 0.06984285, 0.06473171, 0.06542546, 0.06984285,\n",
       "        0.06473171, 0.06542546, 0.06984285, 0.06473171, 0.06542546,\n",
       "        0.06984285, 0.06473171, 0.06542546, 0.06984285, 0.06473171],\n",
       "       dtype=float32)>, <tf.Tensor: id=80458, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06221454, 0.07096781, 0.06681766, 0.06221454, 0.07096781,\n",
       "        0.06681766, 0.06221454, 0.07096781, 0.06681766, 0.06221454,\n",
       "        0.07096781, 0.06681766, 0.06221454, 0.07096781, 0.06681766],\n",
       "       dtype=float32)>, <tf.Tensor: id=83050, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06420484, 0.07014287, 0.06565228, 0.06420484, 0.07014287,\n",
       "        0.06565228, 0.06420484, 0.07014287, 0.06565228, 0.06420484,\n",
       "        0.07014287, 0.06565228, 0.06420484, 0.07014287, 0.06565228],\n",
       "       dtype=float32)>, <tf.Tensor: id=85642, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06594123, 0.06487959, 0.06917919, 0.06594123, 0.06487959,\n",
       "        0.06917919, 0.06594123, 0.06487959, 0.06917919, 0.06594123,\n",
       "        0.06487959, 0.06917919, 0.06594123, 0.06487959, 0.06917919],\n",
       "       dtype=float32)>, <tf.Tensor: id=88234, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06802748, 0.0663173 , 0.06565523, 0.06802748, 0.0663173 ,\n",
       "        0.06565523, 0.06802748, 0.0663173 , 0.06565523, 0.06802748,\n",
       "        0.0663173 , 0.06565523, 0.06802748, 0.0663173 , 0.06565523],\n",
       "       dtype=float32)>, <tf.Tensor: id=90826, shape=(15,), dtype=float32, numpy=\n",
       " array([0.0665856 , 0.06734879, 0.0660656 , 0.0665856 , 0.06734879,\n",
       "        0.0660656 , 0.0665856 , 0.06734879, 0.0660656 , 0.0665856 ,\n",
       "        0.06734879, 0.0660656 , 0.0665856 , 0.06734879, 0.0660656 ],\n",
       "       dtype=float32)>, <tf.Tensor: id=93418, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06387483, 0.06982901, 0.06629616, 0.06387483, 0.06982901,\n",
       "        0.06629616, 0.06387483, 0.06982901, 0.06629616, 0.06387483,\n",
       "        0.06982901, 0.06629616, 0.06387483, 0.06982901, 0.06629616],\n",
       "       dtype=float32)>, <tf.Tensor: id=96010, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06748754, 0.06640652, 0.06610594, 0.06748754, 0.06640652,\n",
       "        0.06610594, 0.06748754, 0.06640652, 0.06610594, 0.06748754,\n",
       "        0.06640652, 0.06610594, 0.06748754, 0.06640652, 0.06610594],\n",
       "       dtype=float32)>, <tf.Tensor: id=98602, shape=(15,), dtype=float32, numpy=\n",
       " array([0.07493244, 0.06556999, 0.05949757, 0.07493244, 0.06556999,\n",
       "        0.05949757, 0.07493244, 0.06556999, 0.05949757, 0.07493244,\n",
       "        0.06556999, 0.05949757, 0.07493244, 0.06556999, 0.05949757],\n",
       "       dtype=float32)>, <tf.Tensor: id=101194, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06662937, 0.06673653, 0.06663411, 0.06662937, 0.06673653,\n",
       "        0.06663411, 0.06662937, 0.06673653, 0.06663411, 0.06662937,\n",
       "        0.06673653, 0.06663411, 0.06662937, 0.06673653, 0.06663411],\n",
       "       dtype=float32)>, <tf.Tensor: id=103786, shape=(15,), dtype=float32, numpy=\n",
       " array([0.06220635, 0.06213917, 0.07565448, 0.06220635, 0.06213917,\n",
       "        0.07565448, 0.06220635, 0.06213917, 0.07565448, 0.06220635,\n",
       "        0.06213917, 0.07565448, 0.06220635, 0.06213917, 0.07565448],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Heads_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([0.1,0.8,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 2. , 3. , 3.5])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.convolve(a,b,'same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Read and Write Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_prev = tf.nn.softmax(w_prev, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=106392, shape=(8,), dtype=float32, numpy=\n",
       "array([0.44150448, 0.53284335, 0.5331958 , 0.4082237 , 0.35997802,\n",
       "       0.49569654, 0.49706772, 0.44460157], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReadWriteVectorGeneration.ReadVector(M_t,w_prev[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_t = tf.sigmoid(tf.random.uniform((HEADS,M)))\n",
    "a_t = tf.sigmoid(tf.random.uniform((HEADS,M)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_Heads_w = All_Heads_W[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, write_head_w in enumerate(write_Heads_w):\n",
    "    M_t = ReadWriteVectorGeneration.WriteOnMemory(M_t, write_head_w, e_t[20 + i], a_t[20 + i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 ns ± 42.1 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dict(a=2,cat=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.5 ns ± 0.449 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit {'a':2, 'cat':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.uniform((M,))\n",
    "a_list = []\n",
    "for i in range(40):\n",
    "    a_list.append(tf.random.uniform((M,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=107545, shape=(8,), dtype=float32, numpy=\n",
       " array([0.7958343 , 0.2405392 , 0.5578928 , 0.5485002 , 0.87103474,\n",
       "        0.62820005, 0.01486003, 0.44726622], dtype=float32)>,\n",
       " <tf.Tensor: id=107552, shape=(8,), dtype=float32, numpy=\n",
       " array([0.18382478, 0.31391227, 0.32524955, 0.9693028 , 0.87679935,\n",
       "        0.5045861 , 0.80644333, 0.6368617 ], dtype=float32)>,\n",
       " <tf.Tensor: id=107559, shape=(8,), dtype=float32, numpy=\n",
       " array([0.65347934, 0.18584335, 0.5184344 , 0.4089533 , 0.8763708 ,\n",
       "        0.933172  , 0.69516075, 0.7479063 ], dtype=float32)>,\n",
       " <tf.Tensor: id=107566, shape=(8,), dtype=float32, numpy=\n",
       " array([0.32188535, 0.48913968, 0.9399965 , 0.49112034, 0.6137265 ,\n",
       "        0.1685077 , 0.75082386, 0.02874684], dtype=float32)>,\n",
       " <tf.Tensor: id=107573, shape=(8,), dtype=float32, numpy=\n",
       " array([0.67042744, 0.5801518 , 0.6172439 , 0.52459276, 0.4075538 ,\n",
       "        0.29342484, 0.5674629 , 0.9692135 ], dtype=float32)>,\n",
       " <tf.Tensor: id=107580, shape=(8,), dtype=float32, numpy=\n",
       " array([0.83588576, 0.33764005, 0.64463794, 0.7666501 , 0.7334926 ,\n",
       "        0.09158421, 0.41441834, 0.87293386], dtype=float32)>,\n",
       " <tf.Tensor: id=107587, shape=(8,), dtype=float32, numpy=\n",
       " array([0.6760559 , 0.5108565 , 0.602384  , 0.9863976 , 0.26141357,\n",
       "        0.66612875, 0.37905705, 0.9487406 ], dtype=float32)>,\n",
       " <tf.Tensor: id=107594, shape=(8,), dtype=float32, numpy=\n",
       " array([0.4934318 , 0.422171  , 0.22874546, 0.9870329 , 0.43515027,\n",
       "        0.6116328 , 0.16685855, 0.89182293], dtype=float32)>,\n",
       " <tf.Tensor: id=107601, shape=(8,), dtype=float32, numpy=\n",
       " array([0.25623417, 0.43851137, 0.5345434 , 0.9700508 , 0.8246325 ,\n",
       "        0.04798043, 0.37883854, 0.15346122], dtype=float32)>,\n",
       " <tf.Tensor: id=107608, shape=(8,), dtype=float32, numpy=\n",
       " array([0.82942057, 0.8980899 , 0.48869097, 0.01735151, 0.29516852,\n",
       "        0.67047274, 0.41604304, 0.63971376], dtype=float32)>,\n",
       " <tf.Tensor: id=107615, shape=(8,), dtype=float32, numpy=\n",
       " array([0.9560602 , 0.95875955, 0.79697585, 0.40774095, 0.7678282 ,\n",
       "        0.50719464, 0.6030706 , 0.8172293 ], dtype=float32)>,\n",
       " <tf.Tensor: id=107622, shape=(8,), dtype=float32, numpy=\n",
       " array([0.17487359, 0.8147795 , 0.8926276 , 0.38992012, 0.6804378 ,\n",
       "        0.14786303, 0.3688674 , 0.00498784], dtype=float32)>,\n",
       " <tf.Tensor: id=107629, shape=(8,), dtype=float32, numpy=\n",
       " array([0.7984694 , 0.679512  , 0.5843061 , 0.962011  , 0.25054872,\n",
       "        0.5114844 , 0.21104276, 0.05032027], dtype=float32)>,\n",
       " <tf.Tensor: id=107636, shape=(8,), dtype=float32, numpy=\n",
       " array([0.06255484, 0.9881877 , 0.9820491 , 0.7122599 , 0.5269723 ,\n",
       "        0.866567  , 0.08589971, 0.76729035], dtype=float32)>,\n",
       " <tf.Tensor: id=107643, shape=(8,), dtype=float32, numpy=\n",
       " array([0.0636915 , 0.7467779 , 0.33462763, 0.8647349 , 0.528895  ,\n",
       "        0.6551299 , 0.5034666 , 0.97102094], dtype=float32)>,\n",
       " <tf.Tensor: id=107650, shape=(8,), dtype=float32, numpy=\n",
       " array([0.4433533 , 0.6924335 , 0.07815528, 0.55743194, 0.93272567,\n",
       "        0.82026327, 0.74741197, 0.09791517], dtype=float32)>,\n",
       " <tf.Tensor: id=107657, shape=(8,), dtype=float32, numpy=\n",
       " array([0.19894695, 0.36859155, 0.7096118 , 0.82528806, 0.26571047,\n",
       "        0.70729685, 0.43170035, 0.98017657], dtype=float32)>,\n",
       " <tf.Tensor: id=107664, shape=(8,), dtype=float32, numpy=\n",
       " array([0.99402845, 0.9599122 , 0.6603433 , 0.10094011, 0.04400051,\n",
       "        0.6131933 , 0.10776711, 0.6327629 ], dtype=float32)>,\n",
       " <tf.Tensor: id=107671, shape=(8,), dtype=float32, numpy=\n",
       " array([0.6306174 , 0.70685625, 0.2568481 , 0.23163629, 0.9403707 ,\n",
       "        0.62728   , 0.13448954, 0.11221313], dtype=float32)>,\n",
       " <tf.Tensor: id=107678, shape=(8,), dtype=float32, numpy=\n",
       " array([0.51266277, 0.8799484 , 0.95055974, 0.00774968, 0.05998755,\n",
       "        0.5114031 , 0.9335729 , 0.08673263], dtype=float32)>,\n",
       " <tf.Tensor: id=107685, shape=(8,), dtype=float32, numpy=\n",
       " array([0.82237375, 0.07541275, 0.77416074, 0.5704242 , 0.9525255 ,\n",
       "        0.7032305 , 0.55940354, 0.912796  ], dtype=float32)>,\n",
       " <tf.Tensor: id=107692, shape=(8,), dtype=float32, numpy=\n",
       " array([0.79120123, 0.02709484, 0.370036  , 0.711596  , 0.58016956,\n",
       "        0.33295667, 0.698504  , 0.06379974], dtype=float32)>,\n",
       " <tf.Tensor: id=107699, shape=(8,), dtype=float32, numpy=\n",
       " array([0.6376786 , 0.18201005, 0.869733  , 0.70223427, 0.09692502,\n",
       "        0.58105993, 0.71306586, 0.39709377], dtype=float32)>,\n",
       " <tf.Tensor: id=107706, shape=(8,), dtype=float32, numpy=\n",
       " array([0.50117135, 0.32503712, 0.6044742 , 0.9773983 , 0.0481441 ,\n",
       "        0.0591023 , 0.21327186, 0.85558915], dtype=float32)>,\n",
       " <tf.Tensor: id=107713, shape=(8,), dtype=float32, numpy=\n",
       " array([0.48052812, 0.6237333 , 0.23975658, 0.7455851 , 0.6701683 ,\n",
       "        0.6012565 , 0.71340156, 0.04476559], dtype=float32)>,\n",
       " <tf.Tensor: id=107720, shape=(8,), dtype=float32, numpy=\n",
       " array([0.17766643, 0.154639  , 0.43741643, 0.17769325, 0.19979286,\n",
       "        0.95776   , 0.50623286, 0.31732893], dtype=float32)>,\n",
       " <tf.Tensor: id=107727, shape=(8,), dtype=float32, numpy=\n",
       " array([0.36026788, 0.15127325, 0.88372195, 0.09462559, 0.09578729,\n",
       "        0.6814287 , 0.47646034, 0.34079695], dtype=float32)>,\n",
       " <tf.Tensor: id=107734, shape=(8,), dtype=float32, numpy=\n",
       " array([0.88826895, 0.1212225 , 0.8428016 , 0.862254  , 0.8500626 ,\n",
       "        0.20310175, 0.38353825, 0.80740714], dtype=float32)>,\n",
       " <tf.Tensor: id=107741, shape=(8,), dtype=float32, numpy=\n",
       " array([0.40486467, 0.09127355, 0.608686  , 0.9458529 , 0.65246665,\n",
       "        0.72091794, 0.99664354, 0.6977769 ], dtype=float32)>,\n",
       " <tf.Tensor: id=107748, shape=(8,), dtype=float32, numpy=\n",
       " array([0.32900965, 0.14299726, 0.57634556, 0.5122124 , 0.16194975,\n",
       "        0.87173355, 0.87627006, 0.81551445], dtype=float32)>,\n",
       " <tf.Tensor: id=107755, shape=(8,), dtype=float32, numpy=\n",
       " array([0.54668415, 0.43017042, 0.53214705, 0.96904683, 0.1127876 ,\n",
       "        0.9914901 , 0.19627535, 0.10187209], dtype=float32)>,\n",
       " <tf.Tensor: id=107762, shape=(8,), dtype=float32, numpy=\n",
       " array([0.6677731 , 0.2817663 , 0.41554916, 0.29972434, 0.11072934,\n",
       "        0.45540214, 0.66262805, 0.8732728 ], dtype=float32)>,\n",
       " <tf.Tensor: id=107769, shape=(8,), dtype=float32, numpy=\n",
       " array([0.60714686, 0.71759224, 0.26026714, 0.6531118 , 0.08900571,\n",
       "        0.19452775, 0.35081518, 0.7710599 ], dtype=float32)>,\n",
       " <tf.Tensor: id=107776, shape=(8,), dtype=float32, numpy=\n",
       " array([0.21980023, 0.5250776 , 0.3182962 , 0.38161778, 0.5538647 ,\n",
       "        0.73789215, 0.08264816, 0.35438716], dtype=float32)>,\n",
       " <tf.Tensor: id=107783, shape=(8,), dtype=float32, numpy=\n",
       " array([0.5748191 , 0.16912663, 0.77873766, 0.9546517 , 0.6937467 ,\n",
       "        0.54268396, 0.31680632, 0.03619051], dtype=float32)>,\n",
       " <tf.Tensor: id=107790, shape=(8,), dtype=float32, numpy=\n",
       " array([0.30645168, 0.75925314, 0.19254279, 0.03640187, 0.8162209 ,\n",
       "        0.70485675, 0.78113854, 0.16677904], dtype=float32)>,\n",
       " <tf.Tensor: id=107797, shape=(8,), dtype=float32, numpy=\n",
       " array([0.74346614, 0.8693074 , 0.37557507, 0.6758137 , 0.39978814,\n",
       "        0.5511987 , 0.5655    , 0.81536114], dtype=float32)>,\n",
       " <tf.Tensor: id=107804, shape=(8,), dtype=float32, numpy=\n",
       " array([0.8098439 , 0.9874083 , 0.90512645, 0.57942164, 0.16909909,\n",
       "        0.48741698, 0.8184037 , 0.8796861 ], dtype=float32)>,\n",
       " <tf.Tensor: id=107811, shape=(8,), dtype=float32, numpy=\n",
       " array([0.42572284, 0.28984106, 0.7647166 , 0.5465838 , 0.27298558,\n",
       "        0.7513335 , 0.17317343, 0.17131293], dtype=float32)>,\n",
       " <tf.Tensor: id=107818, shape=(8,), dtype=float32, numpy=\n",
       " array([0.51583767, 0.73624873, 0.07134771, 0.7385833 , 0.96325326,\n",
       "        0.02224445, 0.49688125, 0.1870749 ], dtype=float32)>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAbstractRNNCell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mVariableSynchronization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTO\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mVariableAggregation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Adds a new variable to the layer.\n",
       "\n",
       "Arguments:\n",
       "  name: Variable name.\n",
       "  shape: Variable shape. Defaults to scalar if unspecified.\n",
       "  dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
       "  initializer: Initializer instance (callable).\n",
       "  regularizer: Regularizer instance (callable).\n",
       "  trainable: Boolean, whether the variable should be part of the layer's\n",
       "    \"trainable_variables\" (e.g. variables, biases)\n",
       "    or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
       "    Note that `trainable` cannot be `True` if `synchronization`\n",
       "    is set to `ON_READ`.\n",
       "  constraint: Constraint instance (callable).\n",
       "  partitioner: Partitioner to be passed to the `Trackable` API.\n",
       "  use_resource: Whether to use `ResourceVariable`.\n",
       "  synchronization: Indicates when a distributed a variable will be\n",
       "    aggregated. Accepted values are constants defined in the class\n",
       "    `tf.VariableSynchronization`. By default the synchronization is set to\n",
       "    `AUTO` and the current `DistributionStrategy` chooses\n",
       "    when to synchronize. If `synchronization` is set to `ON_READ`,\n",
       "    `trainable` must not be set to `True`.\n",
       "  aggregation: Indicates how a distributed variable will be aggregated.\n",
       "    Accepted values are constants defined in the class\n",
       "    `tf.VariableAggregation`.\n",
       "  **kwargs: Additional keyword arguments. Accepted values are `getter` and\n",
       "    `collections`.\n",
       "\n",
       "Returns:\n",
       "  The created variable. Usually either a `Variable` or `ResourceVariable`\n",
       "  instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
       "  instance is returned.\n",
       "\n",
       "Raises:\n",
       "  RuntimeError: If called with partitioned variable regularization and\n",
       "    eager execution is enabled.\n",
       "  ValueError: When giving unsupported dtype and no initializer or when\n",
       "    trainable has been set to True with synchronization set as `ON_READ`.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.keras.layers.AbstractRNNCell.add_weight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = tf.keras.layers.LSTMCell(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_vector_list = tf.random.uniform((HEADS,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller_output = tf.keras.layers.Dense(8,use_bias=True)(tf.concat([a_list] + read_vector_list,axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=107873, shape=(), dtype=float32, numpy=1.378221>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_vector_list[0][0] + a_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 40, 8])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controller_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = tf.keras.layers.Dense(3*M + 3 + 3)(controller_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=108419, shape=(40, 6), dtype=float32, numpy=\n",
       "array([[ 2.42249340e-01,  4.28915918e-01,  5.22147715e-01,\n",
       "        -1.10394299e+00,  1.17238319e+00, -7.26243258e-01],\n",
       "       [ 7.43943095e-01,  6.48761690e-01, -4.80765283e-01,\n",
       "        -1.05578780e+00, -9.06221420e-02, -1.30426693e+00],\n",
       "       [ 7.61377871e-01,  2.09608167e-01, -2.86324322e-01,\n",
       "        -9.59889054e-01,  6.82249129e-01, -9.17188346e-01],\n",
       "       [ 3.41309160e-02,  4.34828788e-01,  4.65557873e-01,\n",
       "        -7.48184681e-01,  6.64874494e-01, -6.43898547e-01],\n",
       "       [ 2.01101378e-01,  2.95013607e-01,  6.49619699e-01,\n",
       "        -5.43551862e-01,  1.16356039e+00, -4.88495171e-01],\n",
       "       [ 4.66551691e-01,  8.56188685e-02,  3.86032090e-02,\n",
       "        -2.42460132e-01,  2.80426443e-01, -4.97029901e-01],\n",
       "       [ 4.49807823e-01,  4.28324759e-01,  4.98643637e-01,\n",
       "        -6.91228986e-01,  1.01384139e+00, -6.82130098e-01],\n",
       "       [ 8.73904824e-01,  7.17743874e-01, -3.24182093e-01,\n",
       "        -1.52445436e+00,  8.13355803e-01, -1.34729588e+00],\n",
       "       [ 3.84603143e-01,  8.93761635e-01,  2.95349717e-01,\n",
       "        -8.99331927e-01,  3.40575755e-01, -1.05385578e+00],\n",
       "       [ 5.36884740e-02,  1.06466293e-01,  5.38361669e-01,\n",
       "        -8.18633616e-01,  1.45878911e+00, -5.66831708e-01],\n",
       "       [ 5.10384679e-01,  7.70233333e-01,  2.61374533e-01,\n",
       "        -1.55399323e+00,  1.19006419e+00, -1.20745265e+00],\n",
       "       [ 2.16598153e-01,  1.19846368e+00,  7.44734108e-01,\n",
       "        -1.51960528e+00,  1.15508926e+00, -1.01891840e+00],\n",
       "       [ 3.16098005e-01,  9.56448853e-01,  8.33386302e-01,\n",
       "        -1.00754786e+00,  1.20610285e+00, -7.34388888e-01],\n",
       "       [ 3.73916030e-01,  5.63341379e-01,  1.41850024e-01,\n",
       "        -1.08498240e+00,  9.60119128e-01, -1.03970587e+00],\n",
       "       [ 6.03373885e-01,  3.10741127e-01, -3.39321375e-01,\n",
       "        -9.53756034e-01,  4.32542354e-01, -1.05862403e+00],\n",
       "       [ 5.14459968e-01,  4.63336706e-01, -2.50716180e-01,\n",
       "        -1.40032959e+00,  7.15177894e-01, -1.24261463e+00],\n",
       "       [ 4.43141222e-01,  1.82978451e-01,  4.61758412e-02,\n",
       "        -6.31982625e-01,  7.87451804e-01, -8.15757036e-01],\n",
       "       [ 4.34023619e-01,  8.67366850e-01,  4.48028445e-01,\n",
       "        -1.61763537e+00,  1.61349940e+00, -1.06623936e+00],\n",
       "       [ 7.04221249e-01,  6.85944259e-01, -3.56194258e-01,\n",
       "        -1.37599206e+00,  4.14833128e-01, -1.21007085e+00],\n",
       "       [ 1.00575946e-01,  9.85237241e-01,  8.72409105e-01,\n",
       "        -8.10689032e-01,  9.20381606e-01, -6.62153959e-01],\n",
       "       [ 5.91606736e-01, -2.11299539e-01, -3.84159505e-01,\n",
       "        -2.04639331e-01,  1.94975212e-02, -5.51324069e-01],\n",
       "       [ 4.21500683e-01,  1.02257371e+00,  2.89946854e-01,\n",
       "        -1.26234794e+00,  5.84568501e-01, -1.04519379e+00],\n",
       "       [ 1.46822035e-01,  2.70589203e-01,  5.04488885e-01,\n",
       "        -2.22107977e-01,  5.74612379e-01, -3.59264940e-01],\n",
       "       [ 3.12025964e-01,  6.39108062e-01,  5.02465606e-01,\n",
       "        -7.54378736e-01,  9.68950391e-01, -7.31299222e-01],\n",
       "       [ 7.66935110e-01,  8.97617221e-01, -7.32062012e-02,\n",
       "        -1.72540760e+00,  9.43118274e-01, -1.38187277e+00],\n",
       "       [ 3.41348886e-01,  5.93238652e-01,  1.70433119e-01,\n",
       "        -9.56074953e-01,  7.13246524e-01, -7.52718151e-01],\n",
       "       [ 1.39865369e-01,  4.02298719e-01,  6.03273571e-01,\n",
       "        -4.98037577e-01,  8.90913248e-01, -3.93340230e-01],\n",
       "       [ 5.82114697e-01,  1.13348269e+00,  7.37580657e-01,\n",
       "        -1.30386460e+00,  1.09391534e+00, -1.02161813e+00],\n",
       "       [ 7.92656183e-01,  3.53210837e-01, -3.13124180e-01,\n",
       "        -8.43776941e-01,  3.76472980e-01, -1.13886988e+00],\n",
       "       [ 3.37074697e-01, -1.82078719e-01,  6.89865649e-03,\n",
       "        -1.67315334e-01,  5.49274802e-01, -4.62751299e-01],\n",
       "       [ 4.70637977e-01,  4.78201509e-01, -1.22924559e-01,\n",
       "        -1.42603135e+00,  9.24623013e-01, -1.12895441e+00],\n",
       "       [ 2.77183264e-01,  2.12132931e-03,  9.93650705e-02,\n",
       "         1.26745641e-01,  1.59507155e-01, -2.58763999e-01],\n",
       "       [ 7.07378328e-01,  5.28898656e-01, -1.31625935e-01,\n",
       "        -9.30058956e-01,  6.85053587e-01, -9.40362930e-01],\n",
       "       [ 4.26270843e-01,  2.96749145e-01, -1.22145481e-01,\n",
       "        -9.48310018e-01,  6.50764346e-01, -8.53652656e-01],\n",
       "       [ 7.38922834e-01,  1.11809361e+00,  1.46733224e-03,\n",
       "        -1.51433110e+00,  3.79175246e-01, -1.33171535e+00],\n",
       "       [ 6.68839097e-01,  1.82622820e-01, -4.55161542e-01,\n",
       "        -5.69891632e-01, -2.58927941e-02, -8.16973925e-01],\n",
       "       [ 5.44045687e-01,  3.20661426e-01, -1.65300161e-01,\n",
       "        -7.43214130e-01,  7.07033575e-01, -8.63573492e-01],\n",
       "       [ 2.58418351e-01,  5.57810903e-01,  6.45413816e-01,\n",
       "        -6.16158009e-01,  1.00752735e+00, -6.48976266e-01],\n",
       "       [ 4.94702637e-01,  8.76294136e-01,  2.66980529e-01,\n",
       "        -1.30303097e+00,  8.44743133e-01, -1.11923945e+00],\n",
       "       [ 5.51815033e-01,  8.68748248e-01,  4.09170687e-02,\n",
       "        -1.04498017e+00,  3.14214051e-01, -1.08196890e+00]], dtype=float32)>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Dense(6)(pm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=108475, shape=(24, 40, 8), dtype=float32, numpy=\n",
       "array([[[0.58238673, 0.6871129 , 0.998054  , ..., 0.35948265,\n",
       "         0.49559212, 0.4939419 ],\n",
       "        [0.29837275, 0.7203516 , 0.6919899 , ..., 0.667313  ,\n",
       "         0.9397123 , 0.04958665],\n",
       "        [0.83356595, 0.4872613 , 0.06348872, ..., 0.45554817,\n",
       "         0.16243148, 0.8563956 ],\n",
       "        ...,\n",
       "        [0.6215521 , 0.34550548, 0.7988658 , ..., 0.23126376,\n",
       "         0.8698598 , 0.34368753],\n",
       "        [0.29138434, 0.3371259 , 0.7330675 , ..., 0.8262805 ,\n",
       "         0.9214444 , 0.55424345],\n",
       "        [0.07794499, 0.5211084 , 0.72069216, ..., 0.05690455,\n",
       "         0.19068575, 0.58132994]],\n",
       "\n",
       "       [[0.58238673, 0.6871129 , 0.998054  , ..., 0.35948265,\n",
       "         0.49559212, 0.4939419 ],\n",
       "        [0.29837275, 0.7203516 , 0.6919899 , ..., 0.667313  ,\n",
       "         0.9397123 , 0.04958665],\n",
       "        [0.83356595, 0.4872613 , 0.06348872, ..., 0.45554817,\n",
       "         0.16243148, 0.8563956 ],\n",
       "        ...,\n",
       "        [0.6215521 , 0.34550548, 0.7988658 , ..., 0.23126376,\n",
       "         0.8698598 , 0.34368753],\n",
       "        [0.29138434, 0.3371259 , 0.7330675 , ..., 0.8262805 ,\n",
       "         0.9214444 , 0.55424345],\n",
       "        [0.07794499, 0.5211084 , 0.72069216, ..., 0.05690455,\n",
       "         0.19068575, 0.58132994]],\n",
       "\n",
       "       [[0.58238673, 0.6871129 , 0.998054  , ..., 0.35948265,\n",
       "         0.49559212, 0.4939419 ],\n",
       "        [0.29837275, 0.7203516 , 0.6919899 , ..., 0.667313  ,\n",
       "         0.9397123 , 0.04958665],\n",
       "        [0.83356595, 0.4872613 , 0.06348872, ..., 0.45554817,\n",
       "         0.16243148, 0.8563956 ],\n",
       "        ...,\n",
       "        [0.6215521 , 0.34550548, 0.7988658 , ..., 0.23126376,\n",
       "         0.8698598 , 0.34368753],\n",
       "        [0.29138434, 0.3371259 , 0.7330675 , ..., 0.8262805 ,\n",
       "         0.9214444 , 0.55424345],\n",
       "        [0.07794499, 0.5211084 , 0.72069216, ..., 0.05690455,\n",
       "         0.19068575, 0.58132994]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.58238673, 0.6871129 , 0.998054  , ..., 0.35948265,\n",
       "         0.49559212, 0.4939419 ],\n",
       "        [0.29837275, 0.7203516 , 0.6919899 , ..., 0.667313  ,\n",
       "         0.9397123 , 0.04958665],\n",
       "        [0.83356595, 0.4872613 , 0.06348872, ..., 0.45554817,\n",
       "         0.16243148, 0.8563956 ],\n",
       "        ...,\n",
       "        [0.6215521 , 0.34550548, 0.7988658 , ..., 0.23126376,\n",
       "         0.8698598 , 0.34368753],\n",
       "        [0.29138434, 0.3371259 , 0.7330675 , ..., 0.8262805 ,\n",
       "         0.9214444 , 0.55424345],\n",
       "        [0.07794499, 0.5211084 , 0.72069216, ..., 0.05690455,\n",
       "         0.19068575, 0.58132994]],\n",
       "\n",
       "       [[0.58238673, 0.6871129 , 0.998054  , ..., 0.35948265,\n",
       "         0.49559212, 0.4939419 ],\n",
       "        [0.29837275, 0.7203516 , 0.6919899 , ..., 0.667313  ,\n",
       "         0.9397123 , 0.04958665],\n",
       "        [0.83356595, 0.4872613 , 0.06348872, ..., 0.45554817,\n",
       "         0.16243148, 0.8563956 ],\n",
       "        ...,\n",
       "        [0.6215521 , 0.34550548, 0.7988658 , ..., 0.23126376,\n",
       "         0.8698598 , 0.34368753],\n",
       "        [0.29138434, 0.3371259 , 0.7330675 , ..., 0.8262805 ,\n",
       "         0.9214444 , 0.55424345],\n",
       "        [0.07794499, 0.5211084 , 0.72069216, ..., 0.05690455,\n",
       "         0.19068575, 0.58132994]],\n",
       "\n",
       "       [[0.58238673, 0.6871129 , 0.998054  , ..., 0.35948265,\n",
       "         0.49559212, 0.4939419 ],\n",
       "        [0.29837275, 0.7203516 , 0.6919899 , ..., 0.667313  ,\n",
       "         0.9397123 , 0.04958665],\n",
       "        [0.83356595, 0.4872613 , 0.06348872, ..., 0.45554817,\n",
       "         0.16243148, 0.8563956 ],\n",
       "        ...,\n",
       "        [0.6215521 , 0.34550548, 0.7988658 , ..., 0.23126376,\n",
       "         0.8698598 , 0.34368753],\n",
       "        [0.29138434, 0.3371259 , 0.7330675 , ..., 0.8262805 ,\n",
       "         0.9214444 , 0.55424345],\n",
       "        [0.07794499, 0.5211084 , 0.72069216, ..., 0.05690455,\n",
       "         0.19068575, 0.58132994]]], dtype=float32)>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat([tf.expand_dims(read_vector_list, 0) for _ in range(24)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = tf.TensorArray(dtype=tf.float32, size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconjugate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'transpose'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Transposes `a`.\n",
       "\n",
       "Permutes the dimensions according to `perm`.\n",
       "\n",
       "The returned tensor's dimension i will correspond to the input dimension\n",
       "`perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is\n",
       "the rank of the input tensor. Hence by default, this operation performs a\n",
       "regular matrix transpose on 2-D input Tensors. If conjugate is True and\n",
       "`a.dtype` is either `complex64` or `complex128` then the values of `a`\n",
       "are conjugated and transposed.\n",
       "\n",
       "@compatibility(numpy)\n",
       "In `numpy` transposes are memory-efficient constant time operations as they\n",
       "simply return a new view of the same data with adjusted `strides`.\n",
       "\n",
       "TensorFlow does not support strides, so `transpose` returns a new tensor with\n",
       "the items permuted.\n",
       "@end_compatibility\n",
       "\n",
       "For example:\n",
       "\n",
       "```python\n",
       "x = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
       "tf.transpose(x)  # [[1, 4]\n",
       "                 #  [2, 5]\n",
       "                 #  [3, 6]]\n",
       "\n",
       "# Equivalently\n",
       "tf.transpose(x, perm=[1, 0])  # [[1, 4]\n",
       "                              #  [2, 5]\n",
       "                              #  [3, 6]]\n",
       "\n",
       "# If x is complex, setting conjugate=True gives the conjugate transpose\n",
       "x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],\n",
       "                 [4 + 4j, 5 + 5j, 6 + 6j]])\n",
       "tf.transpose(x, conjugate=True)  # [[1 - 1j, 4 - 4j],\n",
       "                                 #  [2 - 2j, 5 - 5j],\n",
       "                                 #  [3 - 3j, 6 - 6j]]\n",
       "\n",
       "# 'perm' is more useful for n-dimensional tensors, for n > 2\n",
       "x = tf.constant([[[ 1,  2,  3],\n",
       "                  [ 4,  5,  6]],\n",
       "                 [[ 7,  8,  9],\n",
       "                  [10, 11, 12]]])\n",
       "\n",
       "# Take the transpose of the matrices in dimension-0\n",
       "# (this common operation has a shorthand `linalg.matrix_transpose`)\n",
       "tf.transpose(x, perm=[0, 2, 1])  # [[[1,  4],\n",
       "                                 #   [2,  5],\n",
       "                                 #   [3,  6]],\n",
       "                                 #  [[7, 10],\n",
       "                                 #   [8, 11],\n",
       "                                 #   [9, 12]]]\n",
       "```\n",
       "\n",
       "Args:\n",
       "  a: A `Tensor`.\n",
       "  perm: A permutation of the dimensions of `a`.\n",
       "  conjugate: Optional bool. Setting it to `True` is mathematically equivalent\n",
       "    to tf.math.conj(tf.transpose(input)).\n",
       "  name: A name for the operation (optional).\n",
       "\n",
       "Returns:\n",
       "  A transposed `Tensor`.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ta.unstack(tf.transpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta.unstack(tf.transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5,6,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.random.uniform((HEADS,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=108493, shape=(40, 8), dtype=float32, numpy=\n",
       "array([[1.3934572, 1.8415847, 1.5576944, 1.1662494, 1.8862607, 1.3659568,\n",
       "        1.0904926, 1.7833703],\n",
       "       [1.0211182, 1.6074501, 1.8878678, 1.6938491, 1.474513 , 1.6293229,\n",
       "        1.3422519, 1.4611118],\n",
       "       [1.0679487, 1.4180768, 1.5142341, 1.9332721, 1.0428125, 1.1839468,\n",
       "        1.9850836, 1.905674 ],\n",
       "       [1.512944 , 1.4897838, 1.1008905, 1.5514795, 1.2069813, 1.4097531,\n",
       "        1.2336558, 1.4328662],\n",
       "       [1.7728443, 1.8993512, 1.3469207, 1.6982868, 1.7703989, 1.6601863,\n",
       "        1.3111985, 1.68536  ],\n",
       "       [1.4795976, 1.492899 , 1.3387004, 1.2239287, 1.8639324, 1.1606152,\n",
       "        1.3761445, 1.3843462],\n",
       "       [1.970603 , 1.4159588, 1.6090815, 1.4226836, 1.6520628, 1.2641418,\n",
       "        1.9727721, 1.2493689],\n",
       "       [1.6856592, 1.8549848, 1.0699368, 1.9516469, 1.3968012, 1.7385411,\n",
       "        1.822167 , 1.4389442],\n",
       "       [1.8494685, 1.6810184, 1.4817904, 1.7523859, 1.4897743, 1.6572026,\n",
       "        1.669008 , 1.7956218],\n",
       "       [1.8312303, 1.5950592, 1.3790498, 1.8272866, 1.6226628, 1.0934517,\n",
       "        1.9326309, 1.6268353],\n",
       "       [1.3500738, 1.0378975, 1.2054392, 1.890036 , 1.9418054, 1.3884134,\n",
       "        1.4920603, 1.2462051],\n",
       "       [1.412819 , 1.7642305, 1.5348151, 1.2349837, 1.7512892, 1.9304826,\n",
       "        1.9988428, 1.6882715],\n",
       "       [1.0347264, 1.8984745, 1.9214563, 1.3001629, 1.1211098, 1.3376718,\n",
       "        1.8847537, 1.9601612],\n",
       "       [1.6929715, 1.4481293, 1.5980225, 1.5863283, 1.7919629, 1.9709562,\n",
       "        1.3645164, 1.1128191],\n",
       "       [1.3207344, 1.8554996, 1.6594123, 1.3118904, 1.6363201, 1.4123302,\n",
       "        1.7381502, 1.8287822],\n",
       "       [1.096548 , 1.1817904, 1.151638 , 1.5903908, 1.8160999, 1.0069368,\n",
       "        1.2452539, 1.5921885],\n",
       "       [1.5804039, 1.9334533, 1.401062 , 1.0404333, 1.0049647, 1.204876 ,\n",
       "        1.9473135, 1.1549779],\n",
       "       [1.6151036, 1.995143 , 1.0912929, 1.6096967, 1.6654569, 1.4189924,\n",
       "        1.1262105, 1.5305632],\n",
       "       [1.2646464, 1.4308335, 1.7261225, 1.8177515, 1.8437998, 1.2492158,\n",
       "        1.5474582, 1.8192482],\n",
       "       [1.0274522, 1.2057569, 1.4345747, 1.1149253, 1.7300507, 1.823942 ,\n",
       "        1.3199632, 1.9414221],\n",
       "       [1.9006937, 1.1851443, 1.5097873, 1.9360662, 1.2090893, 1.5722839,\n",
       "        1.0904253, 1.7996845],\n",
       "       [1.0518758, 1.9365075, 1.8803036, 1.2531891, 1.3603951, 1.3083588,\n",
       "        1.7875285, 1.1605712],\n",
       "       [1.958991 , 1.809908 , 1.0281582, 1.7449365, 1.1558908, 1.2143711,\n",
       "        1.554719 , 1.0496534],\n",
       "       [1.7206653, 1.8917751, 1.1943277, 1.0200074, 1.4076002, 1.1397578,\n",
       "        1.8814552, 1.0117154],\n",
       "       [1.1227599, 1.8283532, 1.5241772, 1.1328368, 1.8880606, 1.5291411,\n",
       "        1.5229354, 1.0603758],\n",
       "       [1.5444309, 1.9632505, 1.0014855, 1.9278187, 1.9634501, 1.4400458,\n",
       "        1.4835403, 1.592974 ],\n",
       "       [1.3853732, 1.1916631, 1.3084697, 1.822983 , 1.0141791, 1.4122746,\n",
       "        1.5742221, 1.2288529],\n",
       "       [1.668664 , 1.5562981, 1.674671 , 1.3017911, 1.1346705, 1.3372567,\n",
       "        1.5496764, 1.9816648],\n",
       "       [1.9902953, 1.5057206, 1.9493119, 1.331603 , 1.1724396, 1.8430283,\n",
       "        1.8667876, 1.8387781],\n",
       "       [1.1213284, 1.6616659, 1.4266082, 1.6265987, 1.2717144, 1.6697512,\n",
       "        1.6533737, 1.6504138],\n",
       "       [1.288184 , 1.3890684, 1.9295863, 1.8724388, 1.8697041, 1.9303203,\n",
       "        1.911078 , 1.3841423],\n",
       "       [1.7402475, 1.1820364, 1.4919288, 1.5642556, 1.3105674, 1.3776767,\n",
       "        1.1752269, 1.2869933],\n",
       "       [1.5647535, 1.4280343, 1.0265312, 1.7362567, 1.9611716, 1.6961203,\n",
       "        1.4796205, 1.6272435],\n",
       "       [1.3635608, 1.0748786, 1.8421515, 1.2187456, 1.8919395, 1.5986174,\n",
       "        1.8316082, 1.8043547],\n",
       "       [1.6615639, 1.8028487, 1.9148206, 1.9012358, 1.422518 , 1.327083 ,\n",
       "        1.2314184, 1.8158119],\n",
       "       [1.9566787, 1.0494945, 1.3741562, 1.3908242, 1.8011719, 1.8495904,\n",
       "        1.4989765, 1.4977411],\n",
       "       [1.6601279, 1.5016873, 1.6782662, 1.0271626, 1.5225211, 1.7002319,\n",
       "        1.6040331, 1.1255747],\n",
       "       [1.4234496, 1.415479 , 1.5929788, 1.1400182, 1.7031933, 1.5787157,\n",
       "        1.4880041, 1.3721337],\n",
       "       [1.6715082, 1.5045257, 1.2658015, 1.2696437, 1.5013396, 1.710788 ,\n",
       "        1.6392587, 1.4632494],\n",
       "       [1.3153249, 1.707664 , 1.5322276, 1.6512246, 1.0372785, 1.6006825,\n",
       "        1.242753 , 1.5388892]], dtype=float32)>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat([1] + b, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand(x, dim, N):\n",
    "    return tf.concat([tf.expand_dims(x, dim) for _ in range(N)], axis=dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r_vector_list = [_expand(tf.nn.tanh(init_r[i]), dim=0, N=39) for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_r = [tf.random.uniform((M,)) for _ in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r_vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=114962, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ],\n",
       "        [0.3643731 , 0.5444552 , 0.34602585, 0.37885317, 0.5664864 ,\n",
       "         0.68334967, 0.6807079 , 0.6617872 ]], dtype=float32)>,\n",
       " <tf.Tensor: id=115043, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ],\n",
       "        [0.6059294 , 0.69284534, 0.4309795 , 0.12458444, 0.06251884,\n",
       "         0.13616525, 0.21285668, 0.7133114 ]], dtype=float32)>,\n",
       " <tf.Tensor: id=115124, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594],\n",
       "        [0.53016466, 0.7142894 , 0.31134954, 0.37730688, 0.47079164,\n",
       "         0.34372717, 0.5753124 , 0.61138594]], dtype=float32)>,\n",
       " <tf.Tensor: id=115205, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554],\n",
       "        [0.12533158, 0.5727483 , 0.05811026, 0.7306383 , 0.5916864 ,\n",
       "         0.7164187 , 0.64443624, 0.08082554]], dtype=float32)>,\n",
       " <tf.Tensor: id=115286, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742],\n",
       "        [0.06460014, 0.33981645, 0.35275388, 0.6510304 , 0.6399959 ,\n",
       "         0.68400383, 0.6469373 , 0.26187742]], dtype=float32)>,\n",
       " <tf.Tensor: id=115367, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ],\n",
       "        [0.07100439, 0.6420866 , 0.44611654, 0.5591107 , 0.16261093,\n",
       "         0.5128825 , 0.46732506, 0.3293643 ]], dtype=float32)>,\n",
       " <tf.Tensor: id=115448, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ],\n",
       "        [0.6585508 , 0.11314622, 0.76007456, 0.3301876 , 0.3105154 ,\n",
       "         0.09183293, 0.6226855 , 0.6652711 ]], dtype=float32)>,\n",
       " <tf.Tensor: id=115529, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976],\n",
       "        [0.7364873 , 0.49626994, 0.73360866, 0.55556375, 0.6523341 ,\n",
       "         0.11886296, 0.6064427 , 0.48379976]], dtype=float32)>,\n",
       " <tf.Tensor: id=115610, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626],\n",
       "        [0.0298752 , 0.49437416, 0.30253243, 0.21596898, 0.29371402,\n",
       "         0.6254044 , 0.46005496, 0.36719626]], dtype=float32)>,\n",
       " <tf.Tensor: id=115691, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265],\n",
       "        [0.33714455, 0.75968856, 0.72979116, 0.5759651 , 0.3815223 ,\n",
       "         0.49583572, 0.7586844 , 0.37030265]], dtype=float32)>,\n",
       " <tf.Tensor: id=115772, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076],\n",
       "        [0.03075475, 0.6520504 , 0.0141803 , 0.3803386 , 0.71716046,\n",
       "         0.53847206, 0.6396551 , 0.50982076]], dtype=float32)>,\n",
       " <tf.Tensor: id=115853, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302],\n",
       "        [0.6176188 , 0.45757735, 0.66335183, 0.74675936, 0.57396877,\n",
       "         0.5234461 , 0.0394409 , 0.42539302]], dtype=float32)>,\n",
       " <tf.Tensor: id=115934, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ],\n",
       "        [0.44310433, 0.690123  , 0.683567  , 0.4886263 , 0.43499348,\n",
       "         0.55329615, 0.22682717, 0.6508591 ]], dtype=float32)>,\n",
       " <tf.Tensor: id=116015, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ],\n",
       "        [0.34328923, 0.44287312, 0.33395016, 0.24840039, 0.5665301 ,\n",
       "         0.5546193 , 0.62535506, 0.5432645 ]], dtype=float32)>,\n",
       " <tf.Tensor: id=116096, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188],\n",
       "        [0.7068551 , 0.18811665, 0.43537742, 0.04833469, 0.0153135 ,\n",
       "         0.5400321 , 0.48017123, 0.25566188]], dtype=float32)>,\n",
       " <tf.Tensor: id=116177, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ],\n",
       "        [0.11919638, 0.15317032, 0.73070717, 0.19159308, 0.17005946,\n",
       "         0.10830089, 0.16197139, 0.2586054 ]], dtype=float32)>,\n",
       " <tf.Tensor: id=116258, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ],\n",
       "        [0.09450532, 0.69670326, 0.36638334, 0.17861255, 0.6532854 ,\n",
       "         0.07943571, 0.29075247, 0.4690179 ]], dtype=float32)>,\n",
       " <tf.Tensor: id=116339, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294],\n",
       "        [0.5344041 , 0.6760761 , 0.0185634 , 0.5839605 , 0.54290915,\n",
       "         0.07542942, 0.3524961 , 0.07869294]], dtype=float32)>,\n",
       " <tf.Tensor: id=116420, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877],\n",
       "        [0.6559973 , 0.30939406, 0.0445356 , 0.33746108, 0.6328103 ,\n",
       "         0.48021594, 0.69923663, 0.55895877]], dtype=float32)>,\n",
       " <tf.Tensor: id=116501, shape=(39, 8), dtype=float32, numpy=\n",
       " array([[0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ],\n",
       "        [0.6613872 , 0.26669604, 0.42898738, 0.72540885, 0.7537738 ,\n",
       "         0.53935015, 0.53167486, 0.6180195 ]], dtype=float32)>]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=116613, shape=(54, 15, 8), dtype=float32, numpy=\n",
       "array([[[0.5734388 , 0.7597851 , 0.623947  , ..., 0.7366561 ,\n",
       "         0.52591425, 0.73811144],\n",
       "        [0.70416963, 0.7093603 , 0.6100299 , ..., 0.5819585 ,\n",
       "         0.7151722 , 0.6356709 ],\n",
       "        [0.69485813, 0.5263842 , 0.71951   , ..., 0.7503891 ,\n",
       "         0.64536816, 0.7427778 ],\n",
       "        ...,\n",
       "        [0.6784373 , 0.57028264, 0.72560006, ..., 0.5346641 ,\n",
       "         0.6853796 , 0.64712423],\n",
       "        [0.75006056, 0.6676223 , 0.66696376, ..., 0.63237935,\n",
       "         0.66819984, 0.5410638 ],\n",
       "        [0.6897298 , 0.67942226, 0.64429367, ..., 0.67885107,\n",
       "         0.70504624, 0.62211007]],\n",
       "\n",
       "       [[0.5734388 , 0.7597851 , 0.623947  , ..., 0.7366561 ,\n",
       "         0.52591425, 0.73811144],\n",
       "        [0.70416963, 0.7093603 , 0.6100299 , ..., 0.5819585 ,\n",
       "         0.7151722 , 0.6356709 ],\n",
       "        [0.69485813, 0.5263842 , 0.71951   , ..., 0.7503891 ,\n",
       "         0.64536816, 0.7427778 ],\n",
       "        ...,\n",
       "        [0.6784373 , 0.57028264, 0.72560006, ..., 0.5346641 ,\n",
       "         0.6853796 , 0.64712423],\n",
       "        [0.75006056, 0.6676223 , 0.66696376, ..., 0.63237935,\n",
       "         0.66819984, 0.5410638 ],\n",
       "        [0.6897298 , 0.67942226, 0.64429367, ..., 0.67885107,\n",
       "         0.70504624, 0.62211007]],\n",
       "\n",
       "       [[0.5734388 , 0.7597851 , 0.623947  , ..., 0.7366561 ,\n",
       "         0.52591425, 0.73811144],\n",
       "        [0.70416963, 0.7093603 , 0.6100299 , ..., 0.5819585 ,\n",
       "         0.7151722 , 0.6356709 ],\n",
       "        [0.69485813, 0.5263842 , 0.71951   , ..., 0.7503891 ,\n",
       "         0.64536816, 0.7427778 ],\n",
       "        ...,\n",
       "        [0.6784373 , 0.57028264, 0.72560006, ..., 0.5346641 ,\n",
       "         0.6853796 , 0.64712423],\n",
       "        [0.75006056, 0.6676223 , 0.66696376, ..., 0.63237935,\n",
       "         0.66819984, 0.5410638 ],\n",
       "        [0.6897298 , 0.67942226, 0.64429367, ..., 0.67885107,\n",
       "         0.70504624, 0.62211007]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.5734388 , 0.7597851 , 0.623947  , ..., 0.7366561 ,\n",
       "         0.52591425, 0.73811144],\n",
       "        [0.70416963, 0.7093603 , 0.6100299 , ..., 0.5819585 ,\n",
       "         0.7151722 , 0.6356709 ],\n",
       "        [0.69485813, 0.5263842 , 0.71951   , ..., 0.7503891 ,\n",
       "         0.64536816, 0.7427778 ],\n",
       "        ...,\n",
       "        [0.6784373 , 0.57028264, 0.72560006, ..., 0.5346641 ,\n",
       "         0.6853796 , 0.64712423],\n",
       "        [0.75006056, 0.6676223 , 0.66696376, ..., 0.63237935,\n",
       "         0.66819984, 0.5410638 ],\n",
       "        [0.6897298 , 0.67942226, 0.64429367, ..., 0.67885107,\n",
       "         0.70504624, 0.62211007]],\n",
       "\n",
       "       [[0.5734388 , 0.7597851 , 0.623947  , ..., 0.7366561 ,\n",
       "         0.52591425, 0.73811144],\n",
       "        [0.70416963, 0.7093603 , 0.6100299 , ..., 0.5819585 ,\n",
       "         0.7151722 , 0.6356709 ],\n",
       "        [0.69485813, 0.5263842 , 0.71951   , ..., 0.7503891 ,\n",
       "         0.64536816, 0.7427778 ],\n",
       "        ...,\n",
       "        [0.6784373 , 0.57028264, 0.72560006, ..., 0.5346641 ,\n",
       "         0.6853796 , 0.64712423],\n",
       "        [0.75006056, 0.6676223 , 0.66696376, ..., 0.63237935,\n",
       "         0.66819984, 0.5410638 ],\n",
       "        [0.6897298 , 0.67942226, 0.64429367, ..., 0.67885107,\n",
       "         0.70504624, 0.62211007]],\n",
       "\n",
       "       [[0.5734388 , 0.7597851 , 0.623947  , ..., 0.7366561 ,\n",
       "         0.52591425, 0.73811144],\n",
       "        [0.70416963, 0.7093603 , 0.6100299 , ..., 0.5819585 ,\n",
       "         0.7151722 , 0.6356709 ],\n",
       "        [0.69485813, 0.5263842 , 0.71951   , ..., 0.7503891 ,\n",
       "         0.64536816, 0.7427778 ],\n",
       "        ...,\n",
       "        [0.6784373 , 0.57028264, 0.72560006, ..., 0.5346641 ,\n",
       "         0.6853796 , 0.64712423],\n",
       "        [0.75006056, 0.6676223 , 0.66696376, ..., 0.63237935,\n",
       "         0.66819984, 0.5410638 ],\n",
       "        [0.6897298 , 0.67942226, 0.64429367, ..., 0.67885107,\n",
       "         0.70504624, 0.62211007]]], dtype=float32)>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_expand(tf.tanh(M_t), dim=0, N=54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 8)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N,M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 54\n",
    "features = 20\n",
    "inputs = tf.random.uniform((batch_size,features))\n",
    "n_RH = 2\n",
    "N = 120\n",
    "M = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_vector_list = tf.random.uniform((n_RH,batch_size,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=116641, shape=(2, 54, 28), dtype=float32, numpy=\n",
       "array([[[0.1414398 , 0.3842882 , 0.6772479 , ..., 0.969646  ,\n",
       "         0.37744462, 0.9973525 ],\n",
       "        [0.6472951 , 0.72290194, 0.21282566, ..., 0.28720188,\n",
       "         0.19527411, 0.9012029 ],\n",
       "        [0.6755738 , 0.9845519 , 0.398422  , ..., 0.6543113 ,\n",
       "         0.38553083, 0.63730085],\n",
       "        ...,\n",
       "        [0.52165854, 0.16400516, 0.6842307 , ..., 0.17416847,\n",
       "         0.6016805 , 0.9025004 ],\n",
       "        [0.49156213, 0.9763869 , 0.63306665, ..., 0.626781  ,\n",
       "         0.27169216, 0.82595813],\n",
       "        [0.7387738 , 0.97463   , 0.1625737 , ..., 0.07111514,\n",
       "         0.2527045 , 0.82427096]],\n",
       "\n",
       "       [[0.7226045 , 0.33932137, 0.8060051 , ..., 0.25526655,\n",
       "         0.27908754, 0.87092304],\n",
       "        [0.34164107, 0.09027421, 0.08450985, ..., 0.6581018 ,\n",
       "         0.37719667, 0.05956793],\n",
       "        [0.8894453 , 0.50356495, 0.5786587 , ..., 0.43773568,\n",
       "         0.01841354, 0.9084431 ],\n",
       "        ...,\n",
       "        [0.4358889 , 0.88747525, 0.5239773 , ..., 0.5117806 ,\n",
       "         0.8444269 , 0.89210296],\n",
       "        [0.29400373, 0.07338023, 0.17428839, ..., 0.76738477,\n",
       "         0.06488109, 0.3332504 ],\n",
       "        [0.09128261, 0.32118094, 0.48839998, ..., 0.5468149 ,\n",
       "         0.93753576, 0.33917642]]], dtype=float32)>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_r = [tf.random.uniform([M]) for _ in range(n_RH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_vector_list = [_expand(tf.nn.tanh(init_r[i]), dim=0, N=batch_size)\n",
    "                                 for i in range(n_RH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=117017, shape=(54, 28), dtype=float32, numpy=\n",
       " array([[0.1497504 , 0.6949559 , 0.68602085, ..., 0.33641857, 0.13806678,\n",
       "         0.7257407 ],\n",
       "        [0.1497504 , 0.6949559 , 0.68602085, ..., 0.33641857, 0.13806678,\n",
       "         0.7257407 ],\n",
       "        [0.1497504 , 0.6949559 , 0.68602085, ..., 0.33641857, 0.13806678,\n",
       "         0.7257407 ],\n",
       "        ...,\n",
       "        [0.1497504 , 0.6949559 , 0.68602085, ..., 0.33641857, 0.13806678,\n",
       "         0.7257407 ],\n",
       "        [0.1497504 , 0.6949559 , 0.68602085, ..., 0.33641857, 0.13806678,\n",
       "         0.7257407 ],\n",
       "        [0.1497504 , 0.6949559 , 0.68602085, ..., 0.33641857, 0.13806678,\n",
       "         0.7257407 ]], dtype=float32)>,\n",
       " <tf.Tensor: id=117128, shape=(54, 28), dtype=float32, numpy=\n",
       " array([[0.70605516, 0.26320115, 0.3369606 , ..., 0.07776394, 0.48419595,\n",
       "         0.4855323 ],\n",
       "        [0.70605516, 0.26320115, 0.3369606 , ..., 0.07776394, 0.48419595,\n",
       "         0.4855323 ],\n",
       "        [0.70605516, 0.26320115, 0.3369606 , ..., 0.07776394, 0.48419595,\n",
       "         0.4855323 ],\n",
       "        ...,\n",
       "        [0.70605516, 0.26320115, 0.3369606 , ..., 0.07776394, 0.48419595,\n",
       "         0.4855323 ],\n",
       "        [0.70605516, 0.26320115, 0.3369606 , ..., 0.07776394, 0.48419595,\n",
       "         0.4855323 ],\n",
       "        [0.70605516, 0.26320115, 0.3369606 , ..., 0.07776394, 0.48419595,\n",
       "         0.4855323 ]], dtype=float32)>]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=116627, shape=(54, 20), dtype=float32, numpy=\n",
       " array([[0.63553405, 0.82729447, 0.72089565, ..., 0.29759133, 0.02938223,\n",
       "         0.46231174],\n",
       "        [0.27687585, 0.65886426, 0.3074242 , ..., 0.73042274, 0.757058  ,\n",
       "         0.17669225],\n",
       "        [0.0813297 , 0.6152452 , 0.511917  , ..., 0.89991224, 0.65799344,\n",
       "         0.72661006],\n",
       "        ...,\n",
       "        [0.50193846, 0.76605344, 0.24583673, ..., 0.99915147, 0.5454838 ,\n",
       "         0.4732983 ],\n",
       "        [0.4846015 , 0.77663994, 0.78144395, ..., 0.26790655, 0.81085575,\n",
       "         0.8051983 ],\n",
       "        [0.28491294, 0.45997107, 0.10576379, ..., 0.19729412, 0.5613487 ,\n",
       "         0.02292502]], dtype=float32)>]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=117136, shape=(54, 76), dtype=float32, numpy=\n",
       "array([[0.63553405, 0.82729447, 0.72089565, ..., 0.07776394, 0.48419595,\n",
       "        0.4855323 ],\n",
       "       [0.27687585, 0.65886426, 0.3074242 , ..., 0.07776394, 0.48419595,\n",
       "        0.4855323 ],\n",
       "       [0.0813297 , 0.6152452 , 0.511917  , ..., 0.07776394, 0.48419595,\n",
       "        0.4855323 ],\n",
       "       ...,\n",
       "       [0.50193846, 0.76605344, 0.24583673, ..., 0.07776394, 0.48419595,\n",
       "        0.4855323 ],\n",
       "       [0.4846015 , 0.77663994, 0.78144395, ..., 0.07776394, 0.48419595,\n",
       "        0.4855323 ],\n",
       "       [0.28491294, 0.45997107, 0.10576379, ..., 0.07776394, 0.48419595,\n",
       "        0.4855323 ]], dtype=float32)>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat([inputs] + r_vector_list, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
